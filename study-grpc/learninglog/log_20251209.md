- [x] https://grpc.io/docs/guides/custom-backend-metrics/ (! not supported on Python)

        Supported languages so far Go, Java

    - a mechanism that lets inject custom metrics at the server side, and have those metrics consumed at the client side - used to influence custom load-balancing policies.
        - instead of only relying on simple signals (e.g. CPU load), application-specific knowledge can incorporated to make smarter routing/load-balancing decisions.
            - Example: a weighted round-robin LB policy that uses backend-specific metrics (not just number of connections) to decide where to send new requests.
    - follows an open standard: ORCA (Open Request Cost Aggregation).
    - for setups using the xDS protocol, a custom LB policy can be configured to leverage these backend metrics.
    - per-query metrics reporting
        - The server attaches the custom metrics in the trailing metadata when an RPC finishes.
    - out-of-band metrics reporting
        - The server periodically pushes backend metrics to the client independently of specific RPCs.
        - does not send query cost metrics
        - the metrics emission frequency can be configured in the custom load balancing policy.
- [x] https://grpc.io/docs/guides/custom-load-balancing/ (! not supported on Python)

        Supported languages so far Go, Java

    - A gRPC load balancing policy is given a list of server IP addresses by the name resolver.
        - The policy is responsible for
            - maintaining connections (subchannels) to the servers
            - picking a connection to use when an RPC is sent.
    - By default, gRPC uses the `pick_first` policy -> no load balancing. 
        - the client tries each address from the resolver, and uses the first one that connects successfully. 
    - `round_robin`
        - creates connections to all addresses from the resolver and rotates through them for each RPC.
    - it's possible to add custom load balancing policy.
        - Custom load balancing policies may rely on backend metrics / feedback. 
        1. Implement the load balancer interface
        2. Register the implementation in the load balancer registry. 
        3. Define how the policy parses configuration. 
        4. Manage connections (subchannels) to backends. 
        5. Implement a "picker" - fast logic to choose which subchannel to use when an RPC is invoked.
        6. Enable the policy via configuration in the service config.
            - In a setup where there is a service mesh or central control plane, configuration is handled via control-plane docs / xDS support rather than standard service config.
- [x] https://grpc.io/docs/guides/deadlines/
    - A deadline defines a point in time beyond which a client is not willing to wait for a response from a server.
    - Some language APIs use deadline (absolute time) and others use timeout (relative duration).
    - By default, gRPC does not set a deadline - meaning a client could end up waiting forever.
    - It's recommended that clients explicitly set a realistic deadline validated via load testing.
    - If the server fails to respond before the deadline, the RPC ends with status `DEADLINE_EXCEEDED`.
    - When the client's deadline has passed, the server automatically cancels the call (status `CANCELLED`)
    - Some gRPC implementations propagate deadlines automatically (e.g. Java, Go), while in others (e.g. C++) this behavior must be explicitly enabled.
    - To avoid issues due to clocks not being synchronized across servers, gRPC converts a propagated deadline into a remaining-time timeout (by deducting time already elapsed)
- [x] https://github.com/grpc/grpc/tree/master/examples/python/timeout
    - timeout is set in seconds while calling service method https://github.com/grpc/grpc/blob/master/examples/python/timeout/greeter_client.py#L29
    - in reference:
        - https://grpc.github.io/grpc/python/grpc.html#grpc.UnaryUnaryMultiCallable.__call__
        - https://grpc.github.io/grpc/python/grpc.html#grpc.UnaryUnaryMultiCallable.future
        - https://grpc.github.io/grpc/python/grpc.html#grpc.UnaryUnaryMultiCallable.with_call
        - https://grpc.github.io/grpc/python/grpc.html#grpc.UnaryStreamMultiCallable.__call__
        - ... etc.
- [x] https://grpc.io/docs/guides/debugging/ (python is not supported)
    - grpcdebug - a command‑line tool in the gRPC ecosystem for debugging and troubleshooting gRPC services.
    - fetches the internal states of the gRPC library from the application via gRPC protocol and provides UI to browse them
    - what can be observed with grpcdebug:
        - Statistics about RPCs sent or failed on a given gRPC channel. 
        - Results of address resolution. 
        - Active xDS configuration that determines routing of RPCs.
- [x] https://github.com/grpc/grpc/tree/master/examples/python/debug
    - [Channelz](https://github.com/grpc/proposal/blob/master/A14-channelz.md) - the live channel‑tracing feature for gRPC.
        - recommended to turn on for production services
    - configured with two environment variables GRPC_VERBOSITY and GRPC_TRACE. Available values can be found at https://github.com/grpc/grpc/blob/master/doc/environment_variables.md
    - pdb is a debugging tool that is available for Python interpreters natively
        - gRPC Python uses C-Extension under-the-hood, so pdb may not be able to trace through the whole stack.
    - grpc_cli - a tool to interact with gRPC backend 
        - grpcurl - other similar tool
    - adding channelz to server - channelz.add_channelz_servicer - https://github.com/grpc/grpc/blob/master/examples/python/debug/debug_server.py#L59
    - retrieving statistics from server - call GerServers of the ChannelzStub - https://github.com/grpc/grpc/blob/master/examples/python/debug/get_stats.py#L34
    - grpc_channelz module - https://grpc.github.io/grpc/python/grpc_channelz.html#what-is-grpc-channelz
- [x] https://grpc.io/docs/guides/error/
    - gRPC uses a status‑code + optional string error message model to signal whether an RPC succeeded or failed. On success the server returns an `OK` status; on failure it returns one of its defined error status codes.

    - Richer Error Model

            supported in the C++, Go, Java, Python, and Ruby

        - when using Protocol Buffers for the data format, gRPC supports a richer error model that lets the server include additional error details (as protobuf messages) along with the status code and message.
        - extra error details are encoded in protobuf binary form and transmitted as trailing metadata in the RPC response.
        - Trade‑offs / caveats:
            - Error‑detail support may be inconsistent across different language implementations.
            - Proxies, loggers, or intermediate HTTP processors often do not see the trailing metadata.
            - it interferes with head‑of‑line blocking, and HTTP/2 compression efficiency might degrade due to cache‑misses.
            - large error detail payloads could hit protocol limits (like max header size)
- [x] https://github.com/grpc/grpc/tree/master/examples/python/errors
    - server
        - Creating custom error message from - https://github.com/grpc/grpc/blob/master/examples/python/errors/server.py#L43
            - using built-in error message https://github.com/grpc/grpc/blob/master/examples/python/errors/server.py#L36
        - sending error response - context.abort_with_status( - https://github.com/grpc/grpc/blob/master/examples/python/errors/server.py#L61
    - client
        - parsing status - grpc_status.rpc_status.from_call(rpc_error) - https://github.com/grpc/grpc/blob/master/examples/python/errors/client.py#L36
        - using detail.Is to verify error detail type - https://github.com/grpc/grpc/blob/master/examples/python/errors/client.py#L38
        - using detail.Unpack to retrieve error data into message instance https://github.com/grpc/grpc/blob/master/examples/python/errors/client.py#L40
    - gRPC status module https://grpc.github.io/grpc/python/grpc_status.html
- [x] https://grpc.io/docs/guides/flow-control/ (python cannot override this behavior)
    - Flow control is a mechanism to ensure that a receiver of messages does not get overwhelmed by a fast sender.
        - Flow control only applies to streaming RPCs - for unary RPCs it is not relevant. 
        - Some languages allow users to override the default behavior and take explicit control over flow control. 
        - As the receiver reads data, an acknowledgment is returned - indicating that the receiver has more capacity. 
        - when a value is written to a stream, that does not mean that it has gone out over the network. Rather, that it has been passed to the framework for buffering it and sending it to the OS.
    - There's a risk of deadlock if both client and server do synchronous reads
    - There's a risk of deadlock if both client and server use manual flow control - and both try to perform lots of writes without doing reads. 
- [x] https://github.com/grpc/grpc/tree/master/examples/python/flow_control
    - demonstration of flow control for streaming calls
    - using server options to limit bandwidth - https://github.com/grpc/grpc/blob/master/examples/python/flow_control/flow_control_server.py#L64
    - using client channel options to limit bandwidth - https://github.com/grpc/grpc/blob/master/examples/python/flow_control/flow_control_client.py#L45
    - full list of possible options can be found here - https://github.com/grpc/grpc/blob/v1.76.x/include/grpc/impl/channel_arg_names.h
- [x] https://grpc.io/docs/guides/health-checking/
    - gRPC defines a standard service API (`health/v1`) for performing health checks on gRPC servers. 
    - developer must update and maintain the health status of services.
    - on the client side - automatically use health checking against servers it connects to.
    
    - two modes:
        - A unary RPC via the `Check` endpoint
        - A streaming RPC via the `Watch` endpoint
    
    - server
        1. Create a health check service using the health library. 
        2. Add the service to server. 
        3. Whenever the health of a service changes - mark it correspondingly:
            - `"SERVING"` - service is ready to accept requests
            - `"NOT_SERVING"` - service cannot accept requests
            - `""` to represent the health status of the whole server instead of an individual service. 
        4. When server shuts down, the health-check library must be notified so connected clients can be informed. 

    - client
        - configure the channel's service config - by specifying `"healthCheckConfig": { "serviceName": "<service>" }`. 
        - client will additionally call the `Watch` RPC when a connection is established
            - If the health-check call fails with status `UNIMPLEMENTED`, health checking will be disabled. 
            - Otherwise if the call fails, retries will be made (with exponential backoff)
                - the client will not send RPCs to the server until it receives a `SERVING` status for the target service. 
            - If the service becomes unhealthy, the client will stop sending further requests. 
            - When the service becomes healthy again, client can resume sending requests. 

        - Note: Some load-balancing policies may disable health checking when they consider it incompatible (e.g. `pick_first`). 

        - The health status of a service affects the state of the underlying subchannel (i.e. physical connection) - for example:
            - On connect, the subchannel may move from IDLE -> CONNECTING -> READY only if health check returns `SERVING`. 
            - If the health check fails, the subchannel goes to a transient-failure state. 
            - If the connection breaks or times out, subchannel goes back to IDLE. 
- [x] https://github.com/grpc/grpc/tree/master/examples/python/health_checking
    - https://grpc.github.io/grpc/python/grpc_health_checking.html#grpc_health.v1.health.HealthServicer.Watch
    - server
        - https://github.com/grpc/grpc/blob/master/examples/python/health_checking/greeter_server.py#L47
            1. create service using grpc_health.v1.health.HealthServicer
            2. add to server
        - example of changing status of a service (proto_file_name.ServiceName) https://github.com/grpc/grpc/blob/master/examples/python/health_checking/greeter_server.py#L42
    - client
        - create health stub - https://github.com/grpc/grpc/blob/master/examples/python/health_checking/greeter_client.py#L45
        - request and check status - https://github.com/grpc/grpc/blob/master/examples/python/health_checking/greeter_client.py#L34
        - this example does not demonstrate use of healthCheckConfig