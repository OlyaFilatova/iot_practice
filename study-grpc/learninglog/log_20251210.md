- [x] https://grpc.io/docs/guides/interceptors/
    - aka middleware
    - Interceptors are used for implementing logic that is not specific to a single RPC method.
    - The interceptor APIs are different for client-side than server-side.
- [x] https://grpc.io/docs/guides/metadata/
    - implemented using HTTP/2 headers
    - a way for client and server to exchange additional information associated with an RPC.
    - It consists of key-value pairs, sent with RPC requests or responses (initial or final).
        - Keys are ASCII strings (case-insensitive), values can be ASCII or binary.
        - Keys must not start with the prefix grpc- (that prefix is reserved).
    - Servers may limit the size of request headers (default suggestion: ~ 8 KiB).
    - Custom metadata must follow HTTP/2 "Custom-Metadata" format (binary metadata allowed without base64).
- [x] https://github.com/grpc/grpc/tree/master/examples/python/metadata
    - client
        - sending metadata - https://github.com/grpc/grpc/blob/master/examples/python/metadata/metadata_client.py#L33
        - reading trailing metadata - call.trailing_metadata() - https://github.com/grpc/grpc/blob/master/examples/python/metadata/metadata_client.py#L44
        - in reference
            - https://grpc.github.io/grpc/python/grpc.html#grpc.UnaryUnaryMultiCallable.with_call
            - https://grpc.github.io/grpc/python/grpc.html#grpc.Call.trailing_metadata
    - server
        - reading metadata sent by client - context.invocation_metadata() - https://github.com/grpc/grpc/blob/master/examples/python/metadata/metadata_server.py#L28
        - trailing metadata using context.set_trailing_metadata - https://github.com/grpc/grpc/blob/master/examples/python/metadata/metadata_server.py#L31
        - in reference
            - https://grpc.github.io/grpc/python/grpc.html#grpc.ServicerContext.invocation_metadata
            - https://grpc.github.io/grpc/python/grpc.html#grpc.ServicerContext.set_trailing_metadata
- [x] https://grpc.io/docs/guides/keepalive/
    - HTTP/2 PING-based keepalives to maintain an HTTP/2 connection even when there is no data being sent, by periodically sending a PING frame.
    - helps detect broken connections more quickly than relying on TCP-level timeouts.
    - Some implementations may enable socket-level timeout (e.g. `TCP_USER_TIMEOUT` on Linux) when keepalive is enabled, using value of `KEEPALIVE_TIMEOUT`.
    - clients and servers should agree on keepalive settings. If a server does not support keepalives it may close the connection sending a `GOAWAY` with debug info `"too_many_pings"`.
    - avoid enabling keepalive without calls
    - avoid configuring client's keepalive much below one minute.
- [x] https://github.com/grpc/grpc/tree/master/examples/python/keep_alive
    - server
        - set options on server - https://github.com/grpc/grpc/blob/master/examples/python/keep_alive/greeter_server.py#L66
    - client
        - set options on channel - https://github.com/grpc/grpc/blob/master/examples/python/keep_alive/greeter_client.py#L56
- [x] https://github.com/grpc/grpc/tree/master/examples/python/interceptors
    - headers example
        - client
            - Client interceptor that supports all types of RPCs - https://github.com/grpc/grpc/blob/master/examples/python/interceptors/headers/generic_client_interceptor.py#L19
            - Callback that is sent to it - https://github.com/grpc/grpc/blob/master/examples/python/interceptors/headers/header_manipulator_client_interceptor.py#L32
            - Adding interceptor to the channel - https://github.com/grpc/grpc/blob/master/examples/python/interceptors/headers/greeter_client.py#L36
        - server
            - Server interceptor - https://github.com/grpc/grpc/blob/master/examples/python/interceptors/headers/request_header_validator_interceptor.py#L26
            - Adding interceptor to the server - https://github.com/grpc/grpc/blob/master/examples/python/interceptors/headers/greeter_server.py#L41
    - default_value example
        - example of interceptor that sets default response when an exception is received - https://github.com/grpc/grpc/blob/master/examples/python/interceptors/default_value/default_value_client_interceptor.py#L58
    - async example
        - creating async interceptor - https://github.com/grpc/grpc/blob/master/examples/python/interceptors/async/async_greeter_server_with_interceptor.py#L28
- [x] https://grpc.io/docs/guides/opentelemetry-metrics/
    - gRPC provides support for an OpenTelemetry (OTel) plugin
    - Metrics from gRPC + OTel help with troubleshooting, performance tuning, monitoring, and alerting.
    - instrumented components
        - Client per-call (default, stable): measures full RPC calls 
            - latency
        - Client per-attempt (stable, on by default): tracks each attempt
            - number of attempts started
            - time taken to complete an RPC attempt
            - bytes sent per RPC attempt
            - bytes received per RPC attempt
        - Client per-call retry (experimental)
            - number of retries
            - number of transparent retries
            - number of hedges
            - total time of delay
        - Server-side calls
            - number of RPCs started
            - total bytes sent
            - total bytes received
            - end2end time an RPC takes
        - LB Policy
            - "weighted round robin" (experimental)
                - Number of scheduler updates in which there were not enough endpoints with valid weight, which caused the WRR policy to fall back to RR behavior.
                - Number of endpoints from each scheduler update that don't yet have usable weight information (i.e., either the load report has not yet been received, or it is within the blackout period).
                - Number of endpoints from each scheduler update whose latest weight is older than the expiration period.
                - Weight of an endpoint
            - "pick-first" (experimental)
                - Number of times the selected subchannel becomes disconnected.
                - Number of successful connection attempts.
                - Number of failed connection attempts.
        - xDS-client (experimental)
            - Whether or not the xDS client currently has a working ADS stream to the xDS server.
            - A counter of xDS servers going from healthy to unhealthy.
            - A counter of resources received that were considered valid, even if unchanged.
            - A counter of resources received that were considered invalid.
            - Number of xDS resources.
    - Some metrics/instruments are off by default and must be explicitly enabled via the gRPC OpenTelemetry plugin API.
- [x] https://github.com/grpc/grpc/tree/master/examples/python/observability
    - server
        1. create OTel exporter with `open_telemetry_exporter.OTelMetricExporter` - https://github.com/grpc/grpc/blob/master/examples/python/observability/observability_greeter_server.py#L41
        2. create metric reader with `opentelemetry.sdk.metrics.export.PeriodicExportingMetricReader`
        3. create meter provider with `opentelemetry.sdk.metrics.MeterProvider`
        4. create plugin with `grpc_observability.OpenTelemetryPlugin`
        5. register globally before creating server with `otel_plugin.register_global`
        6. deregister globally after stopping server with `otel_plugin.deregister_global`
    - client (uses same methods as the server)
        1. create OTel exporter - https://github.com/grpc/grpc/blob/master/examples/python/observability/observability_greeter_client.py#L33
        2. create metric reader
        3. create meter provider
        4. create provider
        5. register globally before creating channel
        6. deregister globally after closing channel
- [x] https://grpc.io/docs/guides/performance/
    - Re-use stubs and channels whenever possible.
    - Use keepalive pings for HTTP/2 connections during idle periods - so initial RPCs after inactivity don't suffer delay.
    - Use streaming RPCs for long-lived logical flows of data between client/server.
        - BUT: streaming RPCs have trade-offs: once started they can't be load-balanced; they may be harder to debug; at small scale they may improve performance, but for high load / scalability they may hurt performance or complicate logic - so they should be used only when the application logic benefits significantly.
    - Because each HTTP/2 connection used by a channel limits the number of concurrent streams, if many RPCs (or long-lived streams) saturate those streams, extra RPCs will be queued - potentially causing performance bottlenecks.
        - Workarounds: either create a separate channel for high-load parts of the application, or maintain a pool of multiple channels (with distinct channel arguments) to spread RPCs over multiple HTTP/2 connections.
    - Python specific
        - streaming RPCs create extra threads to receive (and possibly send) messages - this can make streaming RPCs much slower than unary RPCs
        - Using `asyncio` (async API) may improve performance compared to standard blocking APIs.
        - Avoid using the "future API" in the sync stack, because it spawns an extra thread (adding overhead).
        - There is an experimental single-threaded unary-stream implementation (via channel option `SingleThreadedUnaryStream`) which can reduce message-latency by up to ~7%.
- [x] https://github.com/grpc/grpc/blob/master/examples/python/helloworld/greeter_server_with_reflection.py
    - server
        - using grpc_reflection.v1alpha.reflection.enable_server_reflection add reflection to services before starting server https://github.com/grpc/grpc/blob/master/examples/python/helloworld/greeter_server_with_reflection.py#L37C15-L37C40
- [x] https://grpc.io/docs/guides/reflection/
    - With reflection enabled, clients (or tools) can dynamically discover what services and methods the server supports and the structure (types) of request/response - removing the need for pre-shared `.proto` definitions on the client side.
    - Reflection is *not* enabled by default.
    - If reflection is not exposed by the server, clients or tools that expect reflection will fail with errors when trying to interact using reflection-based discovery.
    - For public or exposed APIs, enabling reflection may be a security concern (you reveal your full service definitions/types).
- [x] https://grpc.io/docs/guides/retry/
    - Retry is a mechanism to re-attempt failed RPCs, helping recover from transient failures (network glitches, temporary server issues) to improve service reliability.
    - For effective retrying:
        - choose which failures should trigger retry
        - define exponential backoff parameters
        - set a limit on number of attempts
        - monitor retry metrics
    - under the hood
        - retry logic tracks the call history so that, if needed, it can replay the call by creating a new attempt ("retry stream") when certain failure conditions are met.
        - A retry happens only if the RPC terminates with a status code matching the retry policy's allowed codes, and if the retry attempt limit hasn't been exceeded - after waiting an exponential backoff delay.
        - Once the response header is received (i.e. RPC is committed), no further retries occur.

    - By default, retry support is enabled in gRPC - but there is no default retry policy. Without a policy, gRPC may still perform a limited "transparent retry" in narrow circumstances.
    - Transparent retry cases:
        - If RPC never leaves the client (client-side failure), gRPC may do unlimited retries.
        - If RPC reaches gRPC server library but isn't seen by server application logic, gRPC may retry once.
    - supported configuration:
        - Max number of retry attempts
        - Exponential backoff
        - Set of retryable status codes
    - example config:
    ```json
    "retryPolicy": {
        "maxAttempts": 4,
        "initialBackoff": "0.1s",
        "maxBackoff": "1s",
        "backoffMultiplier": 2,
        "retryableStatusCodes": ["UNAVAILABLE"]
    }
    ```
    - Retry backoff includes jitter: the backoff delay has Â±20% random variation to avoid many clients retrying synchronously.
    - gRPC supports retry throttling to prevent overloading servers due to many retries:
    ```json
    "retryThrottling": {
        "maxTokens": 10,
        "tokenRatio": 0.1
    }
    ```
    - For each server, client tracks a token count starting at `maxTokens`. Failed RPCs reduce the count by 1; successful RPCs increase it by `tokenRatio`. If token count falls below half of max, retries are paused until recovery.
- [x] https://github.com/grpc/grpc/tree/master/examples/python/retry
    - client
        - set config using options parameter - https://github.com/grpc/grpc/blob/master/examples/python/retry/retry_client.py#L52
- [x] https://grpc.io/docs/guides/server-graceful-stop/ (no python example listed)
    - Graceful shutdown lets a gRPC server stop accepting new RPCs, wait for in-flight RPCs to finish (or until a deadline), then shut down - avoiding abrupt termination of active client calls.
    - the general pattern
        - The server first notifies clients to stop sending new RPCs, and rejects any new RPCs from that point on.
        - Existing in-flight RPCs are allowed to complete normally, or until a specified timeout/deadline is reached.
        - After all remaining RPCs finish (or timeout expires), the server completes shutdown.
        - If the deadline expires before RPCs finish, the server can perform a forceful shutdown to terminate pending calls.
    - It is recommended to accompany graceful shutdown with a timeout mechanism: start graceful shutdown, and separately schedule a forceful shutdown after a reasonable duration - to avoid blocking indefinitely if some RPCs never complete.

    - python example is not provided. Here what is available in the reference:
        1. stop method of the server has grace parameter
            - sync https://grpc.github.io/grpc/python/grpc.html#grpc.Server.stop
            - async https://grpc.github.io/grpc/python/grpc_asyncio.html#grpc.aio.Server.stop
        2. stop method of the async channel has grace parameter https://grpc.github.io/grpc/python/grpc_asyncio.html#grpc.aio.Channel.close
- [x] https://grpc.io/docs/guides/service-config/
    - full service config data structure is documented with a [protobuf definition](https://github.com/grpc/grpc-proto/blob/master/grpc/service_config/service_config.proto)
    - The "service config" lets service owners specify how clients should behave when talking to a given server target.
    - It controls client-side behaviors: load balancing, RPC call behavior (timeouts, retry/hedging), and health checking.
    - Load balancing
        - By default the `pick_first` load balancing policy is utilized, but another policy can be specified (E.g. `round_robin`)
    - Call behavior
        - with `wait-for-ready` enabled: if a client cannot connect to a backend, the RPC will wait instead of failing immediately.
        - a call timeout indicates maximum time the client should wait before giving up on an RPC.
        - a retry policy or a hedging policy (max attempts, backoff settings, retryable status codes or non-fatal codes).
    - Health checking
        - configure clients to perform automatic health checking by specifying a health-check service name.
    - How clients get the service config
        - Via name resolution: the name resolver returns both backend addresses and the associated service config. This is how service owners can distribute configs to many clients.
        - Or programmatically: client applications can explicitly provide the service config (in JSON format), which acts as a default when resolver doesn't supply one - useful for testing or fallback.
- [x] https://grpc.io/docs/guides/status-codes/
    - every RPC in gRPC returns a status composed of an integer code and a string description.
    - applications should only use the values defined on the page.

    - The codes INVALID_ARGUMENT, NOT_FOUND, ALREADY_EXISTS, FAILED_PRECONDITION, ABORTED, OUT_OF_RANGE, and DATA_LOSS are never automatically generated by the gRPC library - they are intended to be returned by user application code.

    - For malformed or invalid input (regardless of system state) -> use INVALID_ARGUMENT.
    - For operations invalid because of system state (e.g. resource existing, state preconditions) -> use FAILED_PRECONDITION, ALREADY_EXISTS, OUT_OF_RANGE, NOT_FOUND depending on scenario.
    - For transient unavailability (e.g. service down / network issues) -> UNAVAILABLE may be used - may be retried.
    - For cancellation by caller -> CANCELLED.
    - For authentication/authorization failures -> UNAUTHENTICATED or PERMISSION_DENIED as appropriate.
- [x] https://grpc.io/docs/guides/wait-for-ready/
    - Wait-for-Ready is a per-RPC option (on the stub) that makes an RPC wait until the channel is ready (i.e. server becomes available) before sending the request, instead of failing immediately.
    - The usual deadline for the RPC still applies - if the deadline expires before the channel becomes ready, the RPC fails.

    - Without Wait-for-Ready (default): if channel is in a failure state (e.g. failed to connect), an RPC immediately fails.
    - With Wait-for-Ready: if channel is IDLE, CONNECTING, or TRANSIENT_FAILURE - the RPC is queued, waiting for channel to reach READY - then sent.
    - If channel is in a permanent failure state, even with Wait-for-Ready, RPC fails.
- [x] https://grpc.io/docs/guides/custom-name-resolution/ (not supported by python)
    - Name resolution in gRPC is how a client determines the network addresses (IP + port) for a service name before sending RPCs.
    - By default, gRPC uses DNS for name resolution.
    - gRPC supports a pluggable name resolver interface (Java, Go)
        - Custom name resolvers are used to augment or replace DNS for service discovery (like service registry, dynamic environments, container orchestration, etc.).
        - Custom resolvers can observe changes (e.g. backend failure, scaling up/down) and update addresses dynamically.
        - Custom resolvers can also supply a service config alongside addresses.

        - Lifecycle of a "Target String" with Custom Resolver
            1. The client registers a "resolver provider" implementation into a global registry.
            2. When the client attempts to connect using a target string with a custom scheme (e.g. `my-resolver:///my-service`), gRPC asks the registry for a resolver for that scheme.
            3. The resolver returns a resolver instance, which gRPC uses to resolve addresses - possibly updating them over time as backend topology changes.
- [x] https://grpc.io/docs/guides/request-hedging/ (not supported by python)
    - Hedging is a retry strategy supported by gRPC: the client sends multiple copies of the same request in parallel (or with slight delay) to different backends, uses the first response, and cancels the rest.
    - Hedging is a technique to reduce tail latency in large scale distributed systems. 
    
    - config
        - `maxAttempts`: maximum number of in-flight requests for that RPC. This field is required; if value > 5, gRPC limits it to 5.
        - `hedgingDelay`: optional - delay before issuing subsequent hedged requests; if omitted, all attempts are sent immediately.
        - `nonFatalStatusCodes`: optional list of gRPC status codes: if a hedged request fails with a code not in this list, all outstanding requests are canceled and that error is returned.

    - under the hood
        - On RPC call with hedging enabled: first request is sent immediately.
        - If no successful response after `hedgingDelay`, next request is sent - and so on - until `maxAttempts` or a successful response.
        - When one of the hedged requests succeeds: client returns that response, and cancels all other in-flight requests.
        - If a hedged request fails with a status in `nonFatalStatusCodes`, hedging continues (next attempt sent immediately).
        - If a hedged request fails with a status outside `nonFatalStatusCodes`, all hedged requests are canceled and that error is returned.
        - If all hedged attempts fail, no further retries - overall RPC fails.
        - gRPC call-level deadline applies across the entire hedging sequence. Once deadline passes, the RPC fails regardless of in-flight hedged attempts.

    - to avoid overloading backends, gRPC supports a throttling mechanism configured via `retryThrottling` in service config.
        - For each server name, client tracks `token_count`, initialized to `maxTokens`.
        - On every failed RPC (with non-fatal status or retry-pushback), `token_count` decrements by 1; on every successful RPC, increases by `tokenRatio`.
        - The first request in a hedged call is always sent; subsequent hedged attempts are only sent if `token_count > maxTokens / 2`. Otherwise they are canceled.

    - A server may send "pushback" metadata in its response to request that the client not issue more hedged attempts.
        - If pushback includes a delay (milliseconds) via metadata key `grpc-retry-pushback-ms`, the next hedged request (if any) will be issued after that delay.
        - If the pushback value is negative or unparsable -> treated as "do not retry." -> no further hedged requests for that call
- [x] https://github.com/grpc/grpc/tree/master/examples/python/no_codegen (experimental API)
    - python can use proto services and messages without generating .py files by using runtime protobuf parsing:
        1. calling grpc.protos_and_services - https://github.com/grpc/grpc/blob/master/examples/python/no_codegen/greeter_server.py#L21
        2. calling grpc.protos and grpc.services - https://github.com/grpc/grpc/blob/master/examples/python/no_codegen/greeter_client.py#L33
    - in reference:
        - https://grpc.github.io/grpc/python/grpc.html#runtime-protobuf-parsing
- [x] https://github.com/grpc/grpc/tree/master/examples/python/wait_for_ready
    - unbounded wait_for_ready
        - use wait_for_ready flag when calling a stub method - https://github.com/grpc/grpc/blob/master/examples/python/wait_for_ready/asyncio_wait_for_ready_example.py#L65
    - wait_for_ready with timeout
        - use wait_for_ready flag when calling a stub method - https://github.com/grpc/grpc/blob/master/examples/python/wait_for_ready/wait_for_ready_with_client_timeout_example_client.py#L68
        - set timeout https://github.com/grpc/grpc/blob/master/examples/python/wait_for_ready/wait_for_ready_with_client_timeout_example_client.py#L81
        - update event on successful connection - https://github.com/grpc/grpc/blob/master/examples/python/wait_for_ready/wait_for_ready_with_client_timeout_example_client.py#L45
        - cancel not started future on timeout - https://github.com/grpc/grpc/blob/master/examples/python/wait_for_ready/wait_for_ready_with_client_timeout_example_client.py#L56
- [x] https://github.com/grpc/grpc/tree/master/examples/python/async_streaming
    - handling streaming responses without blocking the current thread
        1. store reference to stub - self._stub = phone_pb2_grpc.PhoneStub(self._channel) - https://github.com/grpc/grpc/blob/master/examples/python/async_streaming/client.py#L35
        2. consume in separate thread https://github.com/grpc/grpc/blob/master/examples/python/async_streaming/client.py#L87
            ```python
            self._consumer_future = self._executor.submit(
                self._response_watcher, response_iterator
            )
            ```
        3. wait for thread event https://github.com/grpc/grpc/blob/master/examples/python/async_streaming/client.py#L93C14-L93C29
        4. meanwhile iterate through responses https://github.com/grpc/grpc/blob/master/examples/python/async_streaming/client.py#L48
- [x] https://github.com/grpc/grpc/tree/master/examples/python/lb_policies
    - client - changing lb policy to round_robin - https://github.com/grpc/grpc/blob/master/examples/python/lb_policies/greeter_client.py#L28
- [x] https://github.com/grpc/grpc/tree/master/examples/python/multiprocessing
    - sidestep the Python global interpreter lock and achieve true parallelism on multicore systems
    - upon calling fork, any threads in a critical section may leave the state of the gRPC library invalid in the child process. https://www.microsoft.com/en-us/research/wp-content/uploads/2019/04/fork-hotos19.pdf
    - Calling fork without exec in your process is supported before any gRPC servers have been instantiated.
    - server
        1. reserve port https://github.com/grpc/grpc/blob/master/examples/python/multiprocessing/server.py#L106
        2. create process for each core/CPU https://github.com/grpc/grpc/blob/master/examples/python/multiprocessing/server.py#L115
        3. in the process handler
            - set grpc.so_reuseport to 1 https://github.com/grpc/grpc/blob/master/examples/python/multiprocessing/server.py#L73
            - start server with futures.ThreadPoolExecutor - https://github.com/grpc/grpc/blob/master/examples/python/multiprocessing/server.py#L70
    - client
        - create worker pool and run queries - https://github.com/grpc/grpc/blob/master/examples/python/multiprocessing/client.py#L68
- [x] https://github.com/grpc/grpc/tree/master/examples/python/uds
    - demonstrates how gRPC Python can utilize the gRPC Name Resolution mechanism to specify UDS address for clients and servers
    - server
        - add a uds address as a server port - https://github.com/grpc/grpc/blob/master/examples/python/uds/greeter_server.py#L35
    - client
        - create channel using a uds address - https://github.com/grpc/grpc/blob/master/examples/python/uds/greeter_client.py#L28
- [x] https://github.com/grpc/grpc/tree/master/examples/python/xds
    - server
        - create fallback credentials - https://github.com/grpc/grpc/blob/master/examples/python/xds/server.py#L97
        - create xds credentials
        - add secure port
    - client
        - create fallback credentials - https://github.com/grpc/grpc/blob/master/examples/python/xds/client.py#L31
        - create xds credentials
        - create secure channel